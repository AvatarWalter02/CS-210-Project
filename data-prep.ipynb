{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Exposure JSON File: LowExposureDailyAverages.json\n",
      "High Exposure JSON File: HighExposureDailyAverages.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Function to process XML, calculate daily averages, and split into high/low exposure JSON\n",
    "def process_and_save_json(file_path, threshold, last_x_days):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for record in root.findall('Record'):\n",
    "        creation_date = record.get('creationDate')\n",
    "        value = float(record.get('value', 0))\n",
    "        date_only = datetime.strptime(creation_date.split()[0], \"%Y-%m-%d\").date()\n",
    "        data.append((date_only, value))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Date', 'Value'])\n",
    "\n",
    "    # Calculate daily averages\n",
    "    daily_averages = df.groupby('Date')['Value'].mean().reset_index()\n",
    "    daily_averages.columns = ['Date', 'DailyAverage']\n",
    "\n",
    "    # Convert Date column to string for JSON serialization\n",
    "    daily_averages['Date'] = daily_averages['Date'].astype(str)\n",
    "\n",
    "    # Split into high and low exposure based on the threshold\n",
    "    daily_averages = daily_averages[daily_averages['DailyAverage'] > 20]\n",
    "    low_records = daily_averages[daily_averages['DailyAverage'] <= threshold].to_dict(orient='records')\n",
    "    high_records = daily_averages[daily_averages['DailyAverage'] > threshold].to_dict(orient='records')\n",
    "\n",
    "\n",
    "    with open(low_json_path, 'w', encoding='utf-8') as low_file:\n",
    "        json.dump(low_records[-last_x_days::], low_file, indent=4)\n",
    "\n",
    "    with open(high_json_path, 'w', encoding='utf-8') as high_file:\n",
    "        json.dump(high_records[-last_x_days::], high_file, indent=4)\n",
    "\n",
    "    return low_json_path, high_json_path\n",
    "\n",
    "# Define the XML file path and threshold\n",
    "threshold = 80.0  # User-defined threshold\n",
    "last_x_days = 30\n",
    "\n",
    "file_path = 'cleaned_grouped_health_data/Headphone_Audio/HKQuantityTypeIdentifierHeadphoneAudioExposure.xml'\n",
    "\n",
    "low_json_path = 'LowExposureDailyAverages.json'   \n",
    "high_json_path = 'HighExposureDailyAverages.json'\n",
    "\n",
    "# Process the file and save to JSON\n",
    "low_json_path, high_json_path = process_and_save_json(file_path, threshold, last_x_days)\n",
    "\n",
    "print(\"Low Exposure JSON File:\", low_json_path)\n",
    "print(\"High Exposure JSON File:\", high_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file_path = 'cleaned_grouped_health_data/Physical_Activity/HKQuantityTypeIdentifierActiveEnergyBurned.xml'\n",
    "output_json_path = 'ActiveEnergyBurnedDaily.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active Energy Burned JSON File: ActiveEnergyBurnedDaily.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Function to process active energy burned data and save daily averages to JSON\n",
    "def process_active_energy_burned(file_path, output_json_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for record in root.findall('Record'):\n",
    "        creation_date = record.get('creationDate')\n",
    "        value = float(record.get('value', 0))\n",
    "        date_only = datetime.strptime(creation_date.split()[0], \"%Y-%m-%d\").date()\n",
    "        data.append((date_only, value))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Date', 'Value'])\n",
    "\n",
    "    # Calculate daily averages\n",
    "    daily_averages = df.groupby('Date')['Value'].sum().reset_index()\n",
    "    daily_averages.columns = ['Date', 'DailyAverage']\n",
    "\n",
    "    # Convert Date column to string for JSON serialization\n",
    "    daily_averages['Date'] = daily_averages['Date'].astype(str)\n",
    "\n",
    "    # Convert to a list of dictionaries\n",
    "    records = daily_averages.to_dict(orient='records')\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(records, json_file, indent=4)\n",
    "\n",
    "    return output_json_path\n",
    "\n",
    "# Process the file and save to JSON\n",
    "result_json_path = process_active_energy_burned(input_file_path, output_json_path)\n",
    "\n",
    "print(\"Active Energy Burned JSON File:\", result_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = 'cleaned_grouped_health_data/Walking_Metrics/HKQuantityTypeIdentifierWalkingSpeed.xml'\n",
    "output_json_path = 'WalkingSpeedDailyAverages.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walking Speed Weighted Average JSON File: WalkingSpeedDailyAverages.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Function to process walking speed data and calculate weighted daily averages\n",
    "def process_weighted_walking_speed(file_path, output_json_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for record in root.findall('Record'):\n",
    "        start_date = datetime.strptime(record.get('startDate'), \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        end_date = datetime.strptime(record.get('endDate'), \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        speed = float(record.get('value', 0))\n",
    "        duration = (end_date - start_date).total_seconds()  # Duration in seconds\n",
    "        distance = speed * duration  # Distance = Speed * Time\n",
    "        data.append((start_date.date(), distance, duration))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Date', 'Distance', 'Duration'])\n",
    "\n",
    "    # Calculate total distance and total duration for each day\n",
    "    daily_totals = df.groupby('Date').sum().reset_index()\n",
    "\n",
    "    # Calculate the weighted average speed for each day\n",
    "    daily_totals['DailyWeightedAverageSpeed'] = daily_totals['Distance'] / daily_totals['Duration']\n",
    "\n",
    "    # Convert Date column to string for JSON serialization\n",
    "    daily_totals['Date'] = daily_totals['Date'].astype(str)\n",
    "\n",
    "    # Convert to a list of dictionaries\n",
    "    records = daily_totals[['Date', 'DailyWeightedAverageSpeed']].to_dict(orient='records')\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(records, json_file, indent=4)\n",
    "\n",
    "    return output_json_path\n",
    "\n",
    "\n",
    "\n",
    "# Process the file and save to JSON\n",
    "result_json_path = process_weighted_walking_speed(input_file_path, output_json_path)\n",
    "\n",
    "print(\"Walking Speed Weighted Average JSON File:\", result_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file_path = 'cleaned_grouped_health_data/Physical_Activity/HKQuantityTypeIdentifierStepCount.xml'\n",
    "output_json_path = 'DailyStepCount.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Step Count JSON File: DailyStepCount.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Function to process step count data and save daily totals to JSON\n",
    "def process_daily_step_count(file_path, output_json_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for record in root.findall('Record'):\n",
    "        start_date = datetime.strptime(record.get('startDate'), \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        step_count = int(record.get('value', 0))\n",
    "        data.append((start_date.date(), step_count))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Date', 'StepCount'])\n",
    "\n",
    "    # Calculate total step count for each day\n",
    "    daily_totals = df.groupby('Date')['StepCount'].sum().reset_index()\n",
    "    daily_totals.columns = ['Date', 'TotalSteps']\n",
    "\n",
    "    # Convert Date column to string for JSON serialization\n",
    "    daily_totals['Date'] = daily_totals['Date'].astype(str)\n",
    "\n",
    "    # Convert to a list of dictionaries\n",
    "    records = daily_totals.to_dict(orient='records')\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(records, json_file, indent=4)\n",
    "\n",
    "    return output_json_path\n",
    "\n",
    "\n",
    "\n",
    "# Process the file and save to JSON\n",
    "result_json_path = process_daily_step_count(input_file_path, output_json_path)\n",
    "\n",
    "print(\"Daily Step Count JSON File:\", result_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "input_file_path = 'cleaned_grouped_health_data/Physical_Activity/HKQuantityTypeIdentifierBasalEnergyBurned.xml'\n",
    "output_json_path = 'BasalEnergyBurnedDaily.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basal Energy Burned JSON File: BasalEnergyBurnedDaily.json\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Function to process basal energy burned data and save daily totals to JSON\n",
    "def process_basal_energy_burned(file_path, output_json_path):\n",
    "    # Parse the XML file\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    for record in root.findall('Record'):\n",
    "        start_date = datetime.strptime(record.get('startDate'), \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        end_date = datetime.strptime(record.get('endDate'), \"%Y-%m-%d %H:%M:%S %z\")\n",
    "        value = float(record.get('value', 0))\n",
    "        duration = (end_date - start_date).total_seconds() / 3600.0  # Duration in hours\n",
    "        total_energy = value * duration  # Energy burned = value * time\n",
    "        data.append((start_date.date(), total_energy))\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Date', 'TotalEnergy'])\n",
    "\n",
    "    # Calculate total energy burned for each day\n",
    "    daily_totals = df.groupby('Date').sum().reset_index()\n",
    "    daily_totals.columns = ['Date', 'TotalBasalEnergyBurned']\n",
    "\n",
    "    # Convert Date column to string for JSON serialization\n",
    "    daily_totals['Date'] = daily_totals['Date'].astype(str)\n",
    "\n",
    "    # Convert to a list of dictionaries\n",
    "    records = daily_totals.to_dict(orient='records')\n",
    "\n",
    "    # Save results to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(records, json_file, indent=4)\n",
    "\n",
    "    return output_json_path\n",
    "\n",
    "\n",
    "\n",
    "# Process the file and save to JSON\n",
    "result_json_path = process_basal_energy_burned(input_file_path, output_json_path)\n",
    "\n",
    "print(\"Basal Energy Burned JSON File:\", result_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposure dates are merged.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Calculate mean for a given key in a dataset\n",
    "def calculate_mean(data, key):\n",
    "    return mean(entry[key] for entry in data if key in entry and entry[key] is not None)\n",
    "\n",
    "# Merge data based on exposure type\n",
    "def merge_data(exposure_data, other_data, exposure_label):\n",
    "    merged_data = []\n",
    "\n",
    "    # Calculate mean values for missing data\n",
    "    mean_values = {\n",
    "        \"step count\": calculate_mean(other_data[\"step_count\"], \"TotalSteps\"),\n",
    "        \"active energy burned\": calculate_mean(other_data[\"active_energy\"], \"DailyAverage\"),\n",
    "        \"basal energy burned\": calculate_mean(other_data[\"basal_energy\"], \"TotalBasalEnergyBurned\"),\n",
    "        \"walking speed\": calculate_mean(other_data[\"walking_speed\"], \"DailyWeightedAverageSpeed\"),\n",
    "    }\n",
    "\n",
    "    for entry in exposure_data:\n",
    "        date = entry[\"Date\"]\n",
    "        exposure = entry[\"DailyAverage\"]\n",
    "\n",
    "        # Look up values from other datasets\n",
    "        step_count = next((x[\"TotalSteps\"] for x in other_data[\"step_count\"] if x[\"Date\"] == date), mean_values[\"step count\"])\n",
    "        active_energy = next((x[\"DailyAverage\"] for x in other_data[\"active_energy\"] if x[\"Date\"] == date), mean_values[\"active energy burned\"])\n",
    "        basal_energy = next((x[\"TotalBasalEnergyBurned\"] for x in other_data[\"basal_energy\"] if x[\"Date\"] == date), mean_values[\"basal energy burned\"])\n",
    "        walking_speed = next((x[\"DailyWeightedAverageSpeed\"] for x in other_data[\"walking_speed\"] if x[\"Date\"] == date), mean_values[\"walking speed\"])\n",
    "\n",
    "        # Append merged data\n",
    "        merged_data.append({\n",
    "            \"date\": date,\n",
    "            \"exposure\": exposure,\n",
    "            \"step count\": step_count,\n",
    "            \"active energy burned\": active_energy,\n",
    "            \"basal energy burned\": basal_energy,\n",
    "            \"walking speed\": walking_speed,\n",
    "        })\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load all files\n",
    "    low_exposure = load_json(\"LowExposureDailyAverages.json\")\n",
    "    high_exposure = load_json(\"HighExposureDailyAverages.json\")\n",
    "    other_data = {\n",
    "        \"step_count\": load_json(\"DailyStepCount.json\"),\n",
    "        \"active_energy\": load_json(\"ActiveEnergyBurnedDaily.json\"),\n",
    "        \"basal_energy\": load_json(\"BasalEnergyBurnedDaily.json\"),\n",
    "        \"walking_speed\": load_json(\"WalkingSpeedDailyAverages.json\"),\n",
    "    }\n",
    "\n",
    "    # Merge data\n",
    "    low_exposure_merged = merge_data(low_exposure, other_data, \"low\")\n",
    "    high_exposure_merged = merge_data(high_exposure, other_data, \"high\")\n",
    "\n",
    "    # Combine results\n",
    "    combined_data = {\"low_exposure\": low_exposure_merged, \"high_exposure\": high_exposure_merged}\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"MergedExposureData.json\", \"w\") as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "        print(\"Exposure dates are merged.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exposure data merged with imputation flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from statistics import mean\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Calculate mean for a given key in a dataset\n",
    "def calculate_mean(data, key):\n",
    "    return mean(entry[key] for entry in data if key in entry and entry[key] is not None)\n",
    "\n",
    "# Merge data based on exposure type\n",
    "def merge_data(exposure_data, other_data, exposure_label):\n",
    "    merged_data = []\n",
    "\n",
    "    # Calculate mean values for missing data\n",
    "    mean_values = {\n",
    "        \"step count\": calculate_mean(other_data[\"step_count\"], \"TotalSteps\"),\n",
    "        \"active energy burned\": calculate_mean(other_data[\"active_energy\"], \"DailyAverage\"),\n",
    "        \"basal energy burned\": calculate_mean(other_data[\"basal_energy\"], \"TotalBasalEnergyBurned\"),\n",
    "        \"walking speed\": calculate_mean(other_data[\"walking_speed\"], \"DailyWeightedAverageSpeed\"),\n",
    "    }\n",
    "\n",
    "    for entry in exposure_data:\n",
    "        date = entry[\"Date\"]\n",
    "        exposure = entry[\"DailyAverage\"]\n",
    "\n",
    "        # Look up values from other datasets\n",
    "        step_count_entry = next((x for x in other_data[\"step_count\"] if x[\"Date\"] == date), None)\n",
    "        active_energy_entry = next((x for x in other_data[\"active_energy\"] if x[\"Date\"] == date), None)\n",
    "        basal_energy_entry = next((x for x in other_data[\"basal_energy\"] if x[\"Date\"] == date), None)\n",
    "        walking_speed_entry = next((x for x in other_data[\"walking_speed\"] if x[\"Date\"] == date), None)\n",
    "\n",
    "        # Assign values or means with an imputation flag\n",
    "        step_count = step_count_entry[\"TotalSteps\"] if step_count_entry else mean_values[\"step count\"]\n",
    "        step_count_imputed = step_count_entry is None\n",
    "\n",
    "        active_energy = active_energy_entry[\"DailyAverage\"] if active_energy_entry else mean_values[\"active energy burned\"]\n",
    "        active_energy_imputed = active_energy_entry is None\n",
    "\n",
    "        basal_energy = basal_energy_entry[\"TotalBasalEnergyBurned\"] if basal_energy_entry else mean_values[\"basal energy burned\"]\n",
    "        basal_energy_imputed = basal_energy_entry is None\n",
    "\n",
    "        walking_speed = walking_speed_entry[\"DailyWeightedAverageSpeed\"] if walking_speed_entry else mean_values[\"walking speed\"]\n",
    "        walking_speed_imputed = walking_speed_entry is None\n",
    "\n",
    "        # Append merged data with flags\n",
    "        merged_data.append({\n",
    "            \"date\": date,\n",
    "            \"exposure\": exposure,\n",
    "            \"step count\": step_count,\n",
    "            \"step count imputed\": step_count_imputed,\n",
    "            \"active energy burned\": active_energy,\n",
    "            \"active energy burned imputed\": active_energy_imputed,\n",
    "            \"basal energy burned\": basal_energy,\n",
    "            \"basal energy burned imputed\": basal_energy_imputed,\n",
    "            \"walking speed\": walking_speed,\n",
    "            \"walking speed imputed\": walking_speed_imputed,\n",
    "        })\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Load all files\n",
    "    low_exposure = load_json(\"LowExposureDailyAverages.json\")\n",
    "    high_exposure = load_json(\"HighExposureDailyAverages.json\")\n",
    "    other_data = {\n",
    "        \"step_count\": load_json(\"DailyStepCount.json\"),\n",
    "        \"active_energy\": load_json(\"ActiveEnergyBurnedDaily.json\"),\n",
    "        \"basal_energy\": load_json(\"BasalEnergyBurnedDaily.json\"),\n",
    "        \"walking_speed\": load_json(\"WalkingSpeedDailyAverages.json\"),\n",
    "    }\n",
    "\n",
    "    # Merge data\n",
    "    low_exposure_merged = merge_data(low_exposure, other_data, \"low\")\n",
    "    high_exposure_merged = merge_data(high_exposure, other_data, \"high\")\n",
    "\n",
    "    # Combine results\n",
    "    combined_data = {\"low_exposure\": low_exposure_merged, \"high_exposure\": high_exposure_merged}\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"MergedExposureData.json\", \"w\") as f:\n",
    "        json.dump(combined_data, f, indent=4)\n",
    "        print(\"Exposure data merged with imputation flags.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
